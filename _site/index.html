<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>ExpressEdit</title>
    <link rel="stylesheet" href="/assets/css/styles.css">
    <link rel="shortcut icon" href="">
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&family=Lato:ital,wght@0,100;0,300;0,400;0,700;0,900;1,100;1,300;1,400;1,700;1,900&family=Noto+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap" rel="stylesheet">
    <link rel="apple-touch-icon" sizes="180x180" href="/assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="/assets/favicon/site.webmanifest">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
  </head>
  <base target="_blank">
  <body>
    <div>
      <div class="wrapper">
        <h1 style="font-family: 'Open Sans Bold', sans-serif; text-align: center;">
          <span style="vertical-align:middle; color: #5C2A9D; font-variant: small-caps;">ExpressEdit</span>
        </h1>
        <h4 style="text-align: center; font-size: 23px;">Video Editing with Natural Language and Sketching</h4>
        <div class="authors-wrapper">
        
          <div class="author-container">
            <div class="author-image">
              
                <a href="https://x.com/tilekbayb">
                  <img src="/assets/img/bekzat.jpg"/>
                </a>
              
            </div>
            <p>
            
              <a href="https://x.com/tilekbayb">
                Bekzat Tilekbay
              </a>
            
            </p>
            <p>
              KAIST
            </p>
          </div>
        
          <div class="author-container">
            <div class="author-image">
              
                <a href="https://saelyne.com">
                  <img src="/assets/img/saelyne.jpg"/>
                </a>
              
            </div>
            <p>
            
              <a href="https://saelyne.com">
                Saelyne Yang
              </a>
            
            </p>
            <p>
              KAIST
            </p>
          </div>
        
          <div class="author-container">
            <div class="author-image">
              
                <img src="/assets/img/michal.jpg"/>
              
            </div>
            <p>
            
              Michal Lewkowicz
            
            </p>
            <p>
              Yale University
            </p>
          </div>
        
          <div class="author-container">
            <div class="author-image">
              
                <img src="/assets/img/alex.jpeg"/>
              
            </div>
            <p>
            
              Alex Suryapranata
            
            </p>
            <p>
              KAIST
            </p>
          </div>
        
          <div class="author-container">
            <div class="author-image">
              
                <a href="https://juhokim.com">
                  <img src="/assets/img/juho.jpg"/>
                </a>
              
            </div>
            <p>
            
              <a href="https://juhokim.com">
                Juho Kim
              </a>
            
            </p>
            <p>
              KAIST
            </p>
          </div>
        
        </div>
      </div>
      <div class="button-container">
        <a class="button" href="https://kixlab.github.io/website-files/2024/iui2024-ExpressEdit-paper.pdf" target="_blank">
          <span> IUI24 Conference Paper (I1)</span>
          <svg xmlns="http://www.w3.org/2000/svg" height="1em" viewBox="0 0 512 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license (Commercial License) Copyright 2023 Fonticons, Inc. --><path d="M0 64C0 28.7 28.7 0 64 0H224V128c0 17.7 14.3 32 32 32H384V304H176c-35.3 0-64 28.7-64 64V512H64c-35.3 0-64-28.7-64-64V64zm384 64H256V0L384 128zM176 352h32c30.9 0 56 25.1 56 56s-25.1 56-56 56H192v32c0 8.8-7.2 16-16 16s-16-7.2-16-16V448 368c0-8.8 7.2-16 16-16zm32 80c13.3 0 24-10.7 24-24s-10.7-24-24-24H192v48h16zm96-80h32c26.5 0 48 21.5 48 48v64c0 26.5-21.5 48-48 48H304c-8.8 0-16-7.2-16-16V368c0-8.8 7.2-16 16-16zm32 128c8.8 0 16-7.2 16-16V400c0-8.8-7.2-16-16-16H320v96h16zm80-112c0-8.8 7.2-16 16-16h48c8.8 0 16 7.2 16 16s-7.2 16-16 16H448v32h32c8.8 0 16 7.2 16 16s-7.2 16-16 16H448v48c0 8.8-7.2 16-16 16s-16-7.2-16-16V432 368z"/></svg>
        </a>
        <a class="button" href="https://hai-gen.github.io/2024/papers/7484-Tilekbay.pdf" target="_blank">
          <span> HAI-GEN Workshop Paper (I2)</span>
          <svg xmlns="http://www.w3.org/2000/svg" height="1em" viewBox="0 0 512 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license (Commercial License) Copyright 2023 Fonticons, Inc. --><path d="M0 64C0 28.7 28.7 0 64 0H224V128c0 17.7 14.3 32 32 32H384V304H176c-35.3 0-64 28.7-64 64V512H64c-35.3 0-64-28.7-64-64V64zm384 64H256V0L384 128zM176 352h32c30.9 0 56 25.1 56 56s-25.1 56-56 56H192v32c0 8.8-7.2 16-16 16s-16-7.2-16-16V448 368c0-8.8 7.2-16 16-16zm32 80c13.3 0 24-10.7 24-24s-10.7-24-24-24H192v48h16zm96-80h32c26.5 0 48 21.5 48 48v64c0 26.5-21.5 48-48 48H304c-8.8 0-16-7.2-16-16V368c0-8.8 7.2-16 16-16zm32 128c8.8 0 16-7.2 16-16V400c0-8.8-7.2-16-16-16H320v96h16zm80-112c0-8.8 7.2-16 16-16h48c8.8 0 16 7.2 16 16s-7.2 16-16 16H448v32h32c8.8 0 16 7.2 16 16s-7.2 16-16 16H448v48c0 8.8-7.2 16-16 16s-16-7.2-16-16V432 368z"/></svg>
        </a>
        <a class="button-disabled">
          <span> Github (TBA)</span>
          <svg xmlns="http://www.w3.org/2000/svg" height="1em" viewBox="0 0 512 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license (Commercial License) Copyright 2023 Fonticons, Inc. --><path d="M352 0c-12.9 0-24.6 7.8-29.6 19.8s-2.2 25.7 6.9 34.9L370.7 96 201.4 265.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L416 141.3l41.4 41.4c9.2 9.2 22.9 11.9 34.9 6.9s19.8-16.6 19.8-29.6V32c0-17.7-14.3-32-32-32H352zM80 32C35.8 32 0 67.8 0 112V432c0 44.2 35.8 80 80 80H400c44.2 0 80-35.8 80-80V320c0-17.7-14.3-32-32-32s-32 14.3-32 32V432c0 8.8-7.2 16-16 16H80c-8.8 0-16-7.2-16-16V112c0-8.8 7.2-16 16-16H192c17.7 0 32-14.3 32-32s-14.3-32-32-32H80z"/></svg>        </a>
      </div>
    </div>
    <div class="wrapper">
      <hr/>
      <p class="text-left"><span class="sys-name">ExpressEdit</span> is a system that enables editing videos via NL text and sketching on the video frame. Powered by LLM and vision models, the system interprets <strong>(1) temporal, (2) spatial, and (3) operational references in an NL command and spatial references from sketching</strong>. The system implements the interpreted edits, which then the user can iterate on.
<br /><br />
<span class="sys-name">ExpressEdit</span>’s design is motivated by an analysis of <strong>176 multimodal expressions of editing commands</strong> from 10 video editors, which revealed the patterns of use of NL and sketching in describing edit intents.
<br /><br />
We present two iterations of <span class="sys-name">ExpressEdit</span>’s interface and the overview of the pipeline.</p>

<p class="img-right"><img src="/assets/img/animation-v5.gif" alt="Animation of the overview of the Computer Vision &amp; LLM-based pipeline." /></p>

<hr />

<h2 id="system-interface-iteration-1">System Interface (Iteration 1)</h2>

<p class="sys-img"><img src="/assets/img/old-system.png" alt="Main interface of ExpressEdit" /></p>

<p>The first iteration of <span class="sys-name">ExpressEdit</span>’s interface is designed to allow users to (1) express their edit commands through natural language text and sketching on the video and (2) iterate on their edit commands as well as manually manipulate the interpretation results.</p>

<p><br /></p>

<p class="text-left"><span class="sys-name">ExpressEdit</span> supports <strong>Edit List</strong> that allows the user to add a layer on top of the video that contains a set of edits. The user can press the <em>Add</em> button to add a layer and the edit commands issued by the user will be stored under the same layer. 
<br /><br />
To describe the edit command, the user can use the <strong>Edit Description</strong> panel and specify <em>Natural Language Text</em> or <em>Sketch</em> on top of the frame.</p>

<p class="img-right"><img src="/assets/img/old-initiating.gif" alt="The animation that shows initiation of the edit command" /></p>

<p><span class="sys-name">ExpressEdit</span> processes the edit command and automatically suggests several candidates at several moments in the video. It automatically specifies the edit parameters &amp; the location of each edit.</p>

<p class="img-left"><img src="/assets/img/old-examining.gif" alt="The animation that shows the suggested edits and the reasoning" /></p>

<p class="text-right"><br /><br />
<span class="sys-name">ExpressEdit</span> allows users evaluate if the suggestions are appropriate through <strong>Examine</strong> panel that provides the reasoning for the suggestion. It shows the reasoning for the temporal &amp; spatial location of the edit as well as the breakdown of the NL edit command.
<br /><br />
The user can go over the suggestions and either <em>accept</em> them or <em>reject</em> them. The user can manipulate each applied edit manually too.</p>

<hr />

<h2 id="system-interface-iteration-2">System Interface (Iteration 2)</h2>

<p class="sys-img"><img src="/assets/img/new_system.jpg" alt="Main interface of ExpressEdit" /></p>

<p>The second iteration of <span class="sys-name">ExpressEdit</span>’s interface is based on findings from the observational study (N=10) conducted with the first version of the interface.</p>

<p><br /></p>

<p class="text-left">Similar to the first version, <span class="sys-name">ExpressEdit</span> supports <strong>Tabs</strong> that allow the user to add a layer on top of the video by pressing <strong>+</strong>. However, now, the user can see the edit commands they issued as well as the suggested edits in the scrollable panel.</p>

<p class="img-right"><img src="/assets/img/new-initiating.gif" alt="The animation that shows initiation of the edit command" /></p>

<p><span class="sys-name">ExpressEdit</span> returns three types of responses after processing the user’s edit command:</p>

<p><img src="/assets/img/new-panels.png" alt="Three types of panels that ExpressEdit returns" /></p>

<p>(a) The <strong>Examine</strong> panel analyzes the user’s natural language command and shows which parts of the input correspond to the description of temporal location, spatial location, and edit operation type and parameters.</p>

<p>(b) The <strong>Edit result</strong> panel shows the preview of the resulting edit by providing a snapshot of the edit together with explanations on why the segment was selected for the edit to apply.</p>

<p>(c) The <strong>Summary</strong> panel allows the user to select edits the user wants to apply among generated edits. As well as <em>request more edits</em>, <em>move the set of edits to a new tab</em>, or <em>navigate to the previous summary</em></p>

<p class="img-left"><img src="/assets/img/new-examining.gif" alt="The animation that shows the suggested edits and the reasoning" /></p>

<p class="text-right">Users can browse the suggested edits and turn them on/off as they edit the video.</p>

<hr />

<h2 id="pipeline">Pipeline</h2>

<p><span class="sys-name">ExpressEdit</span> is powered by CV &amp; LLM based pipeline that interprets NL &amp; Sketch edit commands. It consists of 2 stages: (1) the offline stage which preprocesses the footage to extract useful metadata; (2) the online stage which interprets the edit commands of the user using the preprocessed metadata.</p>

<h3 id="preprocessing-stage-offline">Preprocessing stage (offline)</h3>

<p><img src="/assets/img/pipeline-offline.gif" alt="The animation that shows the preprocessing stage of the pipeline" /></p>

<p>The pipeline first preprocesses the footage video and extracts textual descriptions of <em>10-second segments</em> and segmentations of each <em>1-second frame</em> of the video.</p>

<p>Textual descriptions consist of (1) <strong>synthetic caption</strong> &amp; <strong>salient objects</strong> by <a href="https://huggingface.co/docs/transformers/main/model_doc/blip-2">BLIP-2</a>, (2) <strong>recognized actions</strong> by <a href="https://github.com/OpenGVLab/InternVideo">InternVideo</a>, and (3) <strong>transcript</strong> from original Youtube video by  <a href="https://github.com/yt-dlp/yt-dlp">yt-dlp</a>.</p>

<p>The pipeline also extracts <strong>segmentations</strong> using <a href="https://github.com/facebookresearch/segment-anything">Segment-Anything</a> excluding the areas that are small.</p>

<h3 id="interpretation-stage-online">Interpretation stage (online)</h3>

<p><img src="/assets/img/pipeline-online.gif" alt="The animation that shows how the pipeline handles the online requests" /></p>

<p>As the user issues an edit command, the pipeline parses the NL command and interprets each of the references (temporal, spatial, and edit) by using the preprocessed metadata with corresponding <a href="https://openai.com/gpt-4">GPT-4</a> prompts.</p>

<p>In particular, we use both <a href="https://openai.com/gpt-4">GPT-4</a> and <a href="https://github.com/openai/CLIP">CLIP</a> to interpret the spatial location as the pipeline may need to consider the user’s sketch and the segmentations from the metadata.</p>

<hr />

<h2 id="bibtex">Bibtex</h2>
<pre>

TBA

</pre>

<hr />

<p class="logos"><a href="https://kixlab.org"><img src="/assets/img/kixlab_logo.png" alt="Logo of KIXLAB" /></a>
<a href="https://kaist.ac.kr"><img src="/assets/img/kaist_logo.png" alt="Logo of KAIST" /></a></p>

<p class="center acknowledgement">This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIT) (NRF-2020R1C1C1007587) and the Institute of Information &amp; Communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korean government (MSIT) (No.2021-0-01347, Video Interaction Technologies Using Object-Oriented Video Modeling).</p>

    </div>
    <div class="footer">
      <div>
        <p class="center credits">
          Template from <a href="https://github.com/kixlab/evallm-website" target="_blank">EvalLM</a> by <a href="https://taesookim.com" target="_blank">tsook</a>. Licensed under MIT License.
          <br/>
          Feel free to borrow the template. We only ask you to keep the credit links above.
        </p>
      </div>
    </div>
    
  </body>
</html>
